python3 -m venv .venv
source .venv/bin/activate
pip install --upgrade pip

pip install -r requirements.txt
python -m app.main inventory/sample.yaml



ğŸ§  EKS Fleet Orchestrator Platform
Multi-Tenant Multi-Cluster Upgrade & GitOps Automation Framework
ğŸ¯ Problem Statement

Our organization operates:

âœ… Multiple tenants

âœ… Each tenant in separate AWS Account

âœ… Each tenant having multiple EKS clusters

âœ… Each cluster running different workloads

âœ… Separate infra GitOps + App GitOps

âœ… Need for:

Requirement	Challenge
Upgrade 100+ clusters	Cannot upgrade manually
Canary rollout	Avoid fleet-wide failure
Parallel upgrade	Reduce downtime
Different apps per cluster	Deployment control
Central monitoring	Multi-cluster visibility
Tenant isolation	Account separation
Cost-optimized lower env upgrade	In-place upgrade

So we built an EKS Fleet Orchestrator Platform.

ğŸ—ï¸ High Level Architecture
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚   Blueprint Config Repo    â”‚
                           â”‚ (Tenant YAML Definitions)  â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                                          â”‚
                                Jenkins Job A
                           (Fleet Upgrade Pipeline)
                                          â”‚
                                          â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚ Python Fleet Orchestrator  â”‚
                           â”‚                            â”‚
                           â”‚ - Canary rollout           â”‚
                           â”‚ - Wave upgrade             â”‚
                           â”‚ - State lock (DynamoDB)    â”‚
                           â”‚ - Failure rollback         â”‚
                           â”‚ - Parallel upgrade         â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                     Trigger Jenkins Job B per cluster
                                          â”‚
                                          â–¼
                         Jenkins Job B (Terraform)
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚                                     â”‚
                  â”‚ terraform init                     â”‚
                  â”‚ terraform plan/apply               â”‚
                  â”‚ cluster_version upgrade           â”‚
                  â”‚ nodegroup AMI upgrade             â”‚
                  â”‚ addon upgrade                     â”‚
                  â”‚ ArgoCD cluster registration       â”‚
                  â”‚                                   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                         ArgoCD GitOps Platform
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ ApplicationSet Controller   â”‚
                   â”‚                             â”‚
                   â”‚ Cluster Labels             â”‚
                   â”‚ Fleet Labels              â”‚
                   â”‚ Env Labels                â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                         Applications Deployed

âš™ï¸ Repository Separation
Repo	Responsibility
infra-modules	Terraform Modules (EKS/VPC/etc)
infra-live	Terraform execution
blueprint-config	Tenant YAML
fleet-orchestrator	Upgrade Automation
gitops-platform	ArgoCD Infra Apps
gitops-apps	Business Apps
ğŸ“„ Tenant Blueprint (Single Source of Truth)

This YAML drives:

Infra Creation

Upgrade

Application Deployment

tenant: tenant-a
env: dev
region: ap-south-1

services:
  eks:
    enabled: true
    config:
      dev-cpu-1:
        fleet: cpu
        is_canary: true
        version: "1.29"
        vpc_name: shared

      dev-cpu-2:
        fleet: cpu
        version: "1.29"

      dev-gpu-1:
        fleet: gpu
        version: "1.29"

ğŸš€ Upgrade Strategy Supported
Strategy	Usage
Blue-Green	Production
In-Place	Lower env
Canary	Fleet safety
Wave Rollout	Parallel execution
Rollback	Auto stop
ğŸŸ¢ Blue-Green Upgrade Flow
Current Cluster (Blue)
        â”‚
        â–¼
Create Green Cluster
        â”‚
Upgrade Control Plane
        â”‚
Upgrade Nodegroups
        â”‚
Upgrade Addons
        â”‚
Register Cluster in ArgoCD
        â”‚
Sync Infra GitOps
        â”‚
Sync App GitOps
        â”‚
Switch Traffic (Route53/ALB)
        â”‚
Destroy Blue

ğŸŸ¡ In-Place Upgrade Flow
Python Orchestrator
        â”‚
Trigger Terraform Job
        â”‚
Upgrade Control Plane
        â”‚
Upgrade Nodegroups
        â”‚
Upgrade Addons
        â”‚
ArgoCD Sync
        â”‚
Done

ğŸ§ª Fleet Upgrade Execution Logic

1ï¸âƒ£ Group clusters by fleet
2ï¸âƒ£ Pick Canary cluster
3ï¸âƒ£ Upgrade Canary
4ï¸âƒ£ Bake Time
5ï¸âƒ£ Rollout in Waves
6ï¸âƒ£ Parallel upgrade per wave
7ï¸âƒ£ Failure stops rollout

ğŸ” State Management

We use DynamoDB:

Purpose
Lock cluster during upgrade
Track upgrade success
Avoid re-upgrade
Resume failed wave
ğŸ“¦ Application Deployment

We use:

âœ… ArgoCD
âœ… ApplicationSet
âœ… Cluster Label Matching

Cluster is registered with labels:

fleet=cpu
env=dev
tenant=tenant-a
gpu=false


ApplicationSet:

matchExpressions:
  - key: fleet
    operator: In
    values:
      - cpu


Apps automatically deploy only to matching clusters.

ğŸ“Š Centralized Monitoring Architecture

Each Cluster:

Prometheus

Grafana

Loki

Remote Write â†’ Central Monitoring Cluster

Cluster Prometheus
        â”‚
Remote Write
        â–¼
Central Prometheus
        â”‚
Grafana Global Dashboard


So from single dashboard:

Cluster Health

Pod Failures

Node Usage

Upgrade status

ğŸ§° Fleet Orchestrator Use Cases

EKS Upgrade

ArgoCD bootstrap

Monitoring rollout

Security patch rollout

AMI rotation

Cluster autoscaler config

Istio rollout

Nodegroup resize

Disaster Recovery

Cluster decommission